{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import helpers\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------- read in entries from all files -------------\n",
    "\n",
    "data_raw = []\n",
    "data_raw_filtered = []\n",
    "data_directory = \"data\"\n",
    "\n",
    "for file in os.listdir(\"data\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        data = helpers.read_file(data_directory + \"/\" + file)\n",
    "        data_raw.append(data)\n",
    "\n",
    "        data_raw_filtered = []\n",
    "for file in data_raw:\n",
    "    for entry in file:\n",
    "        if (\"topics\" in entry) and (\"body\" in entry):\n",
    "            data_raw_filtered.append(entry)\n",
    "\n",
    "# ------------------ create bag of words -----------------\n",
    "\n",
    "data_clean = []\n",
    "topics_has_earn_word = []\n",
    "\n",
    "for entry in data_raw_filtered:\n",
    "    words_list = re.sub(\"[^a-zA-Z]\", \" \", entry['body'])\n",
    "    data_clean.append([w for w in words_list.lower().split()])\n",
    "    if \"earn\" in entry[\"topics\"]:\n",
    "        topics_has_earn_word.append(1)\n",
    "    else:\n",
    "        topics_has_earn_word.append(0)\n",
    "\n",
    "data_clean_train = []\n",
    "\n",
    "for line in data_clean:\n",
    "    data_clean_train.append(\" \".join(line))  # glue all words together into a list of strings\n",
    "\n",
    "\n",
    "# ------------------  'normal' BoW -----------------\n",
    "\n",
    "# --- from hear I change a little the first I chose just 5 articles and the first 40 characters to see how my apraoch works\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None\n",
    "                             )\n",
    "newdata=[]\n",
    "for i in data_clean_train[:5]:\n",
    "    newdata.append(i[:40])\n",
    "train_data_features = vectorizer.fit_transform(newdata)\n",
    "\n",
    "train_data_features_array = train_data_features.toarray()\n",
    "# print train_data_features_array.shape\n",
    "\n",
    "shingles=vectorizer.get_feature_names()\n",
    "print len(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'agriculture', u'argentine', u'board', u'champion', u'computer', u'continued', u'crop', u'department', u'figures', u'grain', u'ha', u'in', u'inc', u'it', u'its', u'of', u'products', u'reported', u'said', u'show', u'showers', u'systems', u'terminal', u'the', u'throughout', u'week']\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# hear I get a list of the 5 articles and  words from vectorizer\n",
    "print vectorizer.get_feature_names()\n",
    "print train_data_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  3.  3.  1.  4.  2.  3.  5.  3.  3.  4.  2.  4.  4.  1.  1.  1.  5.\n",
      "   4.  3.  2.  4.  4.  5.  2.  2.]\n",
      " [ 5.  1.  3.  3.  4.  2.  1.  5.  1.  1.  4.  2.  4.  4.  3.  3.  3.  5.\n",
      "   4.  1.  2.  4.  4.  5.  2.  2.]\n",
      " [ 5.  1.  4.  4.  3.  2.  1.  5.  1.  1.  3.  2.  4.  3.  4.  4.  4.  5.\n",
      "   4.  1.  2.  3.  3.  5.  2.  2.]\n",
      " [ 4.  5.  5.  1.  2.  3.  5.  4.  5.  5.  2.  3.  2.  2.  1.  1.  1.  4.\n",
      "   2.  5.  3.  2.  2.  4.  3.  3.]\n",
      " [ 3.  4.  4.  2.  1.  5.  4.  3.  4.  4.  1.  5.  2.  1.  2.  2.  2.  3.\n",
      "   2.  4.  5.  1.  1.  5.  5.  5.]]\n",
      "\n",
      "\n",
      "Buckets:\n",
      "\n",
      "\n",
      "[u'board'] \n",
      "\n",
      "\n",
      "[u'champion', u'its', u'of', u'products'] \n",
      "\n",
      "\n",
      "[u'agriculture', u'department', u'reported'] \n",
      "\n",
      "\n",
      "[u'inc', u'said'] \n",
      "\n",
      "\n",
      "[u'argentine', u'crop', u'figures', u'grain', u'show'] \n",
      "\n",
      "\n",
      "[u'computer', u'ha', u'it', u'systems', u'terminal'] \n",
      "\n",
      "\n",
      "[u'the'] \n",
      "\n",
      "\n",
      "[u'continued', u'in', u'showers', u'throughout', u'week'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Array form the previus\n",
    "minhashArray=train_data_features_array\n",
    "\n",
    "# get the row and columns of arrray\n",
    "r=np.shape(minhashArray)[0]\n",
    "c=np.shape(minhashArray)[1]\n",
    "#Result=np.zeros((r,c))\n",
    "\n",
    "# Creating the phi size for permutation\n",
    "phiSize=r\n",
    "permutationArray=np.array(range(phiSize))\n",
    "np.random.shuffle(permutationArray)\n",
    "#----------------------------------------\n",
    "\n",
    "Result=np.zeros((r,c))\n",
    "# for creat result I iterate trough rows and each time I shuffle permutationArrya and base of the array I create\n",
    "# a result which obtain base of the first occurance of the 1  (I checked with three articles and not changing permutationArray\n",
    "# and the result was reasonable)\n",
    "for rMinhash in range(r):\n",
    "    np.random.shuffle(permutationArray)\n",
    "    for Col in range(c):\n",
    "        placeFinder=0\n",
    "        for Row in permutationArray:\n",
    "            placeFinder+=1\n",
    "            if minhashArray[Row][Col]==1:\n",
    "                Result[rMinhash][Col]=int(placeFinder)\n",
    "\n",
    "# I show the result her to see how the matrix look like after permutation\n",
    "print Result\n",
    "\n",
    "# now I make a bukket list with the similarity in thier columns\n",
    "bucket=[]\n",
    "bucketList=[]\n",
    "WordColNr1=-1\n",
    "listOfTheWords=vectorizer.get_feature_names()\n",
    "for word in listOfTheWords:\n",
    "    WordColNr1+=1\n",
    "    WordColNr2=-1\n",
    "    bucket=[]\n",
    "    for word in listOfTheWords:\n",
    "        WordColNr2+=1\n",
    "        if np.array_equal(Result[:,WordColNr1],Result[:,WordColNr2]):\n",
    "            bucket.append(word)\n",
    "        \n",
    "    if bucket in bucketList:\n",
    "        bucketList.remove(bucket)\n",
    "    bucketList.append(bucket)\n",
    "        \n",
    "print \"\\n\\nBuckets:\\n\\n\"\n",
    "for article in bucketList:\n",
    "    print article,\"\\n\\n\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
